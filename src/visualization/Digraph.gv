digraph {
	graph [size="122.25,122.25"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2270186190224 [label="
 ()" fillcolor=darkolivegreen1]
	2270186265856 [label=MeanBackward0]
	2270186265904 -> 2270186265856
	2270186265904 [label=AddmmBackward0]
	2270186266096 -> 2270186265904
	2270179223088 [label="head.fc.bias
 (10)" fillcolor=lightblue]
	2270179223088 -> 2270186266096
	2270186266096 [label=AccumulateGrad]
	2270186265520 -> 2270186265904
	2270186265520 [label=ReshapeAliasBackward0]
	2270186265712 -> 2270186265520
	2270186265712 [label=PermuteBackward0]
	2270186265232 -> 2270186265712
	2270186265232 [label=NativeLayerNormBackward0]
	2270186265184 -> 2270186265232
	2270186265184 [label=PermuteBackward0]
	2270186264848 -> 2270186265184
	2270186264848 [label=AsStridedBackward1]
	2270186264800 -> 2270186264848
	2270186264800 [label=MeanBackward1]
	2270186264512 -> 2270186264800
	2270186264512 [label=AddBackward0]
	2270186264464 -> 2270186264512
	2270186264464 [label=MulBackward0]
	2270186264224 -> 2270186264464
	2270186264224 [label=ConvolutionBackward0]
	2270186263984 -> 2270186264224
	2270186263984 [label=GeluBackward0]
	2270186263648 -> 2270186263984
	2270186263648 [label=ConvolutionBackward0]
	2270186263600 -> 2270186263648
	2270186263600 [label=PermuteBackward0]
	2270186263264 -> 2270186263600
	2270186263264 [label=NativeLayerNormBackward0]
	2270186263216 -> 2270186263264
	2270186263216 [label=PermuteBackward0]
	2270186262784 -> 2270186263216
	2270186262784 [label=ConvolutionBackward0]
	2270186264560 -> 2270186262784
	2270186264560 [label=AddBackward0]
	2270186262592 -> 2270186264560
	2270186262592 [label=MulBackward0]
	2270186250000 -> 2270186262592
	2270186250000 [label=ConvolutionBackward0]
	2270186249760 -> 2270186250000
	2270186249760 [label=GeluBackward0]
	2270186249424 -> 2270186249760
	2270186249424 [label=ConvolutionBackward0]
	2270186249376 -> 2270186249424
	2270186249376 [label=PermuteBackward0]
	2270186248944 -> 2270186249376
	2270186248944 [label=NativeLayerNormBackward0]
	2270186248896 -> 2270186248944
	2270186248896 [label=PermuteBackward0]
	2270186248560 -> 2270186248896
	2270186248560 [label=ConvolutionBackward0]
	2270186250144 -> 2270186248560
	2270186250144 [label=ConvolutionBackward0]
	2270186248272 -> 2270186250144
	2270186248272 [label=PermuteBackward0]
	2270186247936 -> 2270186248272
	2270186247936 [label=NativeLayerNormBackward0]
	2270186247888 -> 2270186247936
	2270186247888 [label=PermuteBackward0]
	2270186247552 -> 2270186247888
	2270186247552 [label=AddBackward0]
	2270186247504 -> 2270186247552
	2270186247504 [label=MulBackward0]
	2270186247264 -> 2270186247504
	2270186247264 [label=ConvolutionBackward0]
	2270186247024 -> 2270186247264
	2270186247024 [label=GeluBackward0]
	2270186246688 -> 2270186247024
	2270186246688 [label=ConvolutionBackward0]
	2270186246640 -> 2270186246688
	2270186246640 [label=PermuteBackward0]
	2270186246304 -> 2270186246640
	2270186246304 [label=NativeLayerNormBackward0]
	2270186246256 -> 2270186246304
	2270186246256 [label=PermuteBackward0]
	2270186221280 -> 2270186246256
	2270186221280 [label=ConvolutionBackward0]
	2270186247600 -> 2270186221280
	2270186247600 [label=AddBackward0]
	2270186220992 -> 2270186247600
	2270186220992 [label=MulBackward0]
	2270186220752 -> 2270186220992
	2270186220752 [label=ConvolutionBackward0]
	2270186220512 -> 2270186220752
	2270186220512 [label=GeluBackward0]
	2270186220176 -> 2270186220512
	2270186220176 [label=ConvolutionBackward0]
	2270186220128 -> 2270186220176
	2270186220128 [label=PermuteBackward0]
	2270186219792 -> 2270186220128
	2270186219792 [label=NativeLayerNormBackward0]
	2270186219744 -> 2270186219792
	2270186219744 [label=PermuteBackward0]
	2270186219408 -> 2270186219744
	2270186219408 [label=ConvolutionBackward0]
	2270186221088 -> 2270186219408
	2270186221088 [label=AddBackward0]
	2270186219120 -> 2270186221088
	2270186219120 [label=MulBackward0]
	2270186218880 -> 2270186219120
	2270186218880 [label=ConvolutionBackward0]
	2270186218640 -> 2270186218880
	2270186218640 [label=GeluBackward0]
	2270186218304 -> 2270186218640
	2270186218304 [label=ConvolutionBackward0]
	2270186218256 -> 2270186218304
	2270186218256 [label=PermuteBackward0]
	2270186217920 -> 2270186218256
	2270186217920 [label=NativeLayerNormBackward0]
	2270186217872 -> 2270186217920
	2270186217872 [label=PermuteBackward0]
	2270186217536 -> 2270186217872
	2270186217536 [label=ConvolutionBackward0]
	2270186219216 -> 2270186217536
	2270186219216 [label=AddBackward0]
	2270186200800 -> 2270186219216
	2270186200800 [label=MulBackward0]
	2270186200560 -> 2270186200800
	2270186200560 [label=ConvolutionBackward0]
	2270186200320 -> 2270186200560
	2270186200320 [label=GeluBackward0]
	2270186199984 -> 2270186200320
	2270186199984 [label=ConvolutionBackward0]
	2270186199936 -> 2270186199984
	2270186199936 [label=PermuteBackward0]
	2270186199600 -> 2270186199936
	2270186199600 [label=NativeLayerNormBackward0]
	2270186199552 -> 2270186199600
	2270186199552 [label=PermuteBackward0]
	2270186199216 -> 2270186199552
	2270186199216 [label=ConvolutionBackward0]
	2270186200896 -> 2270186199216
	2270186200896 [label=AddBackward0]
	2270186198928 -> 2270186200896
	2270186198928 [label=MulBackward0]
	2270186198688 -> 2270186198928
	2270186198688 [label=ConvolutionBackward0]
	2270186198448 -> 2270186198688
	2270186198448 [label=GeluBackward0]
	2270186198112 -> 2270186198448
	2270186198112 [label=ConvolutionBackward0]
	2270186198064 -> 2270186198112
	2270186198064 [label=PermuteBackward0]
	2270186197728 -> 2270186198064
	2270186197728 [label=NativeLayerNormBackward0]
	2270186197680 -> 2270186197728
	2270186197680 [label=PermuteBackward0]
	2270186197248 -> 2270186197680
	2270186197248 [label=ConvolutionBackward0]
	2270186199024 -> 2270186197248
	2270186199024 [label=AddBackward0]
	2270186197056 -> 2270186199024
	2270186197056 [label=MulBackward0]
	2270186180368 -> 2270186197056
	2270186180368 [label=ConvolutionBackward0]
	2270186180128 -> 2270186180368
	2270186180128 [label=GeluBackward0]
	2270186179792 -> 2270186180128
	2270186179792 [label=ConvolutionBackward0]
	2270186179744 -> 2270186179792
	2270186179744 [label=PermuteBackward0]
	2270186179312 -> 2270186179744
	2270186179312 [label=NativeLayerNormBackward0]
	2270186179264 -> 2270186179312
	2270186179264 [label=PermuteBackward0]
	2270186178928 -> 2270186179264
	2270186178928 [label=ConvolutionBackward0]
	2270186180512 -> 2270186178928
	2270186180512 [label=ConvolutionBackward0]
	2270186178640 -> 2270186180512
	2270186178640 [label=PermuteBackward0]
	2270186178304 -> 2270186178640
	2270186178304 [label=NativeLayerNormBackward0]
	2270186178256 -> 2270186178304
	2270186178256 [label=PermuteBackward0]
	2270186177920 -> 2270186178256
	2270186177920 [label=AddBackward0]
	2270186177872 -> 2270186177920
	2270186177872 [label=MulBackward0]
	2270186177632 -> 2270186177872
	2270186177632 [label=ConvolutionBackward0]
	2270186177392 -> 2270186177632
	2270186177392 [label=GeluBackward0]
	2270186177056 -> 2270186177392
	2270186177056 [label=ConvolutionBackward0]
	2270186177008 -> 2270186177056
	2270186177008 [label=PermuteBackward0]
	2270186176672 -> 2270186177008
	2270186176672 [label=NativeLayerNormBackward0]
	2270186176624 -> 2270186176672
	2270186176624 [label=PermuteBackward0]
	2270186159744 -> 2270186176624
	2270186159744 [label=ConvolutionBackward0]
	2270186177968 -> 2270186159744
	2270186177968 [label=AddBackward0]
	2270186159456 -> 2270186177968
	2270186159456 [label=MulBackward0]
	2270186159312 -> 2270186159456
	2270186159312 [label=ConvolutionBackward0]
	2270186159072 -> 2270186159312
	2270186159072 [label=GeluBackward0]
	2270186158736 -> 2270186159072
	2270186158736 [label=ConvolutionBackward0]
	2270186158688 -> 2270186158736
	2270186158688 [label=PermuteBackward0]
	2270186158256 -> 2270186158688
	2270186158256 [label=NativeLayerNormBackward0]
	2270186158208 -> 2270186158256
	2270186158208 [label=PermuteBackward0]
	2270186157872 -> 2270186158208
	2270186157872 [label=ConvolutionBackward0]
	2270186159552 -> 2270186157872
	2270186159552 [label=ConvolutionBackward0]
	2270186157584 -> 2270186159552
	2270186157584 [label=PermuteBackward0]
	2270186157248 -> 2270186157584
	2270186157248 [label=NativeLayerNormBackward0]
	2270186157200 -> 2270186157248
	2270186157200 [label=PermuteBackward0]
	2270186156864 -> 2270186157200
	2270186156864 [label=AddBackward0]
	2270186156816 -> 2270186156864
	2270186156816 [label=MulBackward0]
	2270186156576 -> 2270186156816
	2270186156576 [label=ConvolutionBackward0]
	2270186156336 -> 2270186156576
	2270186156336 [label=GeluBackward0]
	2270186156192 -> 2270186156336
	2270186156192 [label=ConvolutionBackward0]
	2270179146240 -> 2270186156192
	2270179146240 [label=PermuteBackward0]
	2270179147728 -> 2270179146240
	2270179147728 [label=NativeLayerNormBackward0]
	2270179147440 -> 2270179147728
	2270179147440 [label=PermuteBackward0]
	2270179147296 -> 2270179147440
	2270179147296 [label=ConvolutionBackward0]
	2270186156912 -> 2270179147296
	2270186156912 [label=AddBackward0]
	2270179146960 -> 2270186156912
	2270179146960 [label=MulBackward0]
	2270179146096 -> 2270179146960
	2270179146096 [label=ConvolutionBackward0]
	2270179146624 -> 2270179146096
	2270179146624 [label=GeluBackward0]
	2270186288512 -> 2270179146624
	2270186288512 [label=ConvolutionBackward0]
	2270186288464 -> 2270186288512
	2270186288464 [label=PermuteBackward0]
	2270186288128 -> 2270186288464
	2270186288128 [label=NativeLayerNormBackward0]
	2270186287984 -> 2270186288128
	2270186287984 [label=PermuteBackward0]
	2270186287648 -> 2270186287984
	2270186287648 [label=ConvolutionBackward0]
	2270179147056 -> 2270186287648
	2270179147056 [label=PermuteBackward0]
	2270186287360 -> 2270179147056
	2270186287360 [label=NativeLayerNormBackward0]
	2270186287168 -> 2270186287360
	2270186287168 [label=PermuteBackward0]
	2270186288848 -> 2270186287168
	2270186288848 [label=ConvolutionBackward0]
	2270186288944 -> 2270186288848
	2270162142848 [label="stem.0.weight
 (40, 3, 4, 4)" fillcolor=lightblue]
	2270162142848 -> 2270186288944
	2270186288944 [label=AccumulateGrad]
	2270186288896 -> 2270186288848
	2270162142928 [label="stem.0.bias
 (40)" fillcolor=lightblue]
	2270162142928 -> 2270186288896
	2270186288896 [label=AccumulateGrad]
	2270186287264 -> 2270186287360
	2272029109840 [label="stem.1.weight
 (40)" fillcolor=lightblue]
	2272029109840 -> 2270186287264
	2270186287264 [label=AccumulateGrad]
	2270186287504 -> 2270186287360
	2270101258048 [label="stem.1.bias
 (40)" fillcolor=lightblue]
	2270101258048 -> 2270186287504
	2270186287504 [label=AccumulateGrad]
	2270186287600 -> 2270186287648
	2270162143248 [label="stages.0.blocks.0.conv_dw.weight
 (40, 1, 7, 7)" fillcolor=lightblue]
	2270162143248 -> 2270186287600
	2270186287600 [label=AccumulateGrad]
	2270186287552 -> 2270186287648
	2270162143328 [label="stages.0.blocks.0.conv_dw.bias
 (40)" fillcolor=lightblue]
	2270162143328 -> 2270186287552
	2270186287552 [label=AccumulateGrad]
	2270186287936 -> 2270186288128
	2270162143088 [label="stages.0.blocks.0.norm.weight
 (40)" fillcolor=lightblue]
	2270162143088 -> 2270186287936
	2270186287936 [label=AccumulateGrad]
	2270186288176 -> 2270186288128
	2270162143408 [label="stages.0.blocks.0.norm.bias
 (40)" fillcolor=lightblue]
	2270162143408 -> 2270186288176
	2270186288176 [label=AccumulateGrad]
	2270186288416 -> 2270186288512
	2270162199552 [label="stages.0.blocks.0.mlp.fc1.weight
 (160, 40, 1, 1)" fillcolor=lightblue]
	2270162199552 -> 2270186288416
	2270186288416 [label=AccumulateGrad]
	2270186288560 -> 2270186288512
	2270162199632 [label="stages.0.blocks.0.mlp.fc1.bias
 (160)" fillcolor=lightblue]
	2270162199632 -> 2270186288560
	2270186288560 [label=AccumulateGrad]
	2270179146576 -> 2270179146096
	2270162199792 [label="stages.0.blocks.0.mlp.fc2.weight
 (40, 160, 1, 1)" fillcolor=lightblue]
	2270162199792 -> 2270179146576
	2270179146576 [label=AccumulateGrad]
	2270179145904 -> 2270179146096
	2270162199872 [label="stages.0.blocks.0.mlp.fc2.bias
 (40)" fillcolor=lightblue]
	2270162199872 -> 2270179145904
	2270179145904 [label=AccumulateGrad]
	2270179146912 -> 2270179146960
	2270179146912 [label=ReshapeAliasBackward0]
	2270179146528 -> 2270179146912
	2270162199712 [label="stages.0.blocks.0.gamma
 (40)" fillcolor=lightblue]
	2270162199712 -> 2270179146528
	2270179146528 [label=AccumulateGrad]
	2270179147056 -> 2270186156912
	2270179147104 -> 2270179147296
	2270162200192 [label="stages.0.blocks.1.conv_dw.weight
 (40, 1, 7, 7)" fillcolor=lightblue]
	2270162200192 -> 2270179147104
	2270179147104 [label=AccumulateGrad]
	2270179147200 -> 2270179147296
	2270162200272 [label="stages.0.blocks.1.conv_dw.bias
 (40)" fillcolor=lightblue]
	2270162200272 -> 2270179147200
	2270179147200 [label=AccumulateGrad]
	2270179147632 -> 2270179147728
	2270162200112 [label="stages.0.blocks.1.norm.weight
 (40)" fillcolor=lightblue]
	2270162200112 -> 2270179147632
	2270179147632 [label=AccumulateGrad]
	2270179146144 -> 2270179147728
	2270162200352 [label="stages.0.blocks.1.norm.bias
 (40)" fillcolor=lightblue]
	2270162200352 -> 2270179146144
	2270179146144 [label=AccumulateGrad]
	2270179146336 -> 2270186156192
	2270162200512 [label="stages.0.blocks.1.mlp.fc1.weight
 (160, 40, 1, 1)" fillcolor=lightblue]
	2270162200512 -> 2270179146336
	2270179146336 [label=AccumulateGrad]
	2270179146288 -> 2270186156192
	2270162200592 [label="stages.0.blocks.1.mlp.fc1.bias
 (160)" fillcolor=lightblue]
	2270162200592 -> 2270179146288
	2270179146288 [label=AccumulateGrad]
	2270186156432 -> 2270186156576
	2270162200752 [label="stages.0.blocks.1.mlp.fc2.weight
 (40, 160, 1, 1)" fillcolor=lightblue]
	2270162200752 -> 2270186156432
	2270186156432 [label=AccumulateGrad]
	2270186156384 -> 2270186156576
	2270162200832 [label="stages.0.blocks.1.mlp.fc2.bias
 (40)" fillcolor=lightblue]
	2270162200832 -> 2270186156384
	2270186156384 [label=AccumulateGrad]
	2270186156672 -> 2270186156816
	2270186156672 [label=ReshapeAliasBackward0]
	2270186156288 -> 2270186156672
	2270162200672 [label="stages.0.blocks.1.gamma
 (40)" fillcolor=lightblue]
	2270162200672 -> 2270186156288
	2270186156288 [label=AccumulateGrad]
	2270186156912 -> 2270186156864
	2270186157296 -> 2270186157248
	2270162200992 [label="stages.1.downsample.0.weight
 (40)" fillcolor=lightblue]
	2270162200992 -> 2270186157296
	2270186157296 [label=AccumulateGrad]
	2270186157536 -> 2270186157248
	2270162201072 [label="stages.1.downsample.0.bias
 (40)" fillcolor=lightblue]
	2270162201072 -> 2270186157536
	2270186157536 [label=AccumulateGrad]
	2270186157680 -> 2270186159552
	2270162201392 [label="stages.1.downsample.1.weight
 (80, 40, 2, 2)" fillcolor=lightblue]
	2270162201392 -> 2270186157680
	2270186157680 [label=AccumulateGrad]
	2270186157632 -> 2270186159552
	2270162201472 [label="stages.1.downsample.1.bias
 (80)" fillcolor=lightblue]
	2270162201472 -> 2270186157632
	2270186157632 [label=AccumulateGrad]
	2270186157824 -> 2270186157872
	2270162201632 [label="stages.1.blocks.0.conv_dw.weight
 (80, 1, 7, 7)" fillcolor=lightblue]
	2270162201632 -> 2270186157824
	2270186157824 [label=AccumulateGrad]
	2270186157920 -> 2270186157872
	2270162201712 [label="stages.1.blocks.0.conv_dw.bias
 (80)" fillcolor=lightblue]
	2270162201712 -> 2270186157920
	2270186157920 [label=AccumulateGrad]
	2270186158304 -> 2270186158256
	2270162201552 [label="stages.1.blocks.0.norm.weight
 (80)" fillcolor=lightblue]
	2270162201552 -> 2270186158304
	2270186158304 [label=AccumulateGrad]
	2270186158544 -> 2270186158256
	2270162201792 [label="stages.1.blocks.0.norm.bias
 (80)" fillcolor=lightblue]
	2270162201792 -> 2270186158544
	2270186158544 [label=AccumulateGrad]
	2270186158784 -> 2270186158736
	2270162201952 [label="stages.1.blocks.0.mlp.fc1.weight
 (320, 80, 1, 1)" fillcolor=lightblue]
	2270162201952 -> 2270186158784
	2270186158784 [label=AccumulateGrad]
	2270186159024 -> 2270186158736
	2270162202032 [label="stages.1.blocks.0.mlp.fc1.bias
 (320)" fillcolor=lightblue]
	2270162202032 -> 2270186159024
	2270186159024 [label=AccumulateGrad]
	2270186159168 -> 2270186159312
	2270162202192 [label="stages.1.blocks.0.mlp.fc2.weight
 (80, 320, 1, 1)" fillcolor=lightblue]
	2270162202192 -> 2270186159168
	2270186159168 [label=AccumulateGrad]
	2270186159120 -> 2270186159312
	2270162202272 [label="stages.1.blocks.0.mlp.fc2.bias
 (80)" fillcolor=lightblue]
	2270162202272 -> 2270186159120
	2270186159120 [label=AccumulateGrad]
	2270186159408 -> 2270186159456
	2270186159408 [label=ReshapeAliasBackward0]
	2270186158496 -> 2270186159408
	2270162202112 [label="stages.1.blocks.0.gamma
 (80)" fillcolor=lightblue]
	2270162202112 -> 2270186158496
	2270186158496 [label=AccumulateGrad]
	2270186159552 -> 2270186177968
	2270186159696 -> 2270186159744
	2270162321472 [label="stages.1.blocks.1.conv_dw.weight
 (80, 1, 7, 7)" fillcolor=lightblue]
	2270162321472 -> 2270186159696
	2270186159696 [label=AccumulateGrad]
	2270186159792 -> 2270186159744
	2270162321552 [label="stages.1.blocks.1.conv_dw.bias
 (80)" fillcolor=lightblue]
	2270162321552 -> 2270186159792
	2270186159792 [label=AccumulateGrad]
	2270186176720 -> 2270186176672
	2270162202512 [label="stages.1.blocks.1.norm.weight
 (80)" fillcolor=lightblue]
	2270162202512 -> 2270186176720
	2270186176720 [label=AccumulateGrad]
	2270186176864 -> 2270186176672
	2270162321632 [label="stages.1.blocks.1.norm.bias
 (80)" fillcolor=lightblue]
	2270162321632 -> 2270186176864
	2270186176864 [label=AccumulateGrad]
	2270186177104 -> 2270186177056
	2270162321792 [label="stages.1.blocks.1.mlp.fc1.weight
 (320, 80, 1, 1)" fillcolor=lightblue]
	2270162321792 -> 2270186177104
	2270186177104 [label=AccumulateGrad]
	2270186177344 -> 2270186177056
	2270162321872 [label="stages.1.blocks.1.mlp.fc1.bias
 (320)" fillcolor=lightblue]
	2270162321872 -> 2270186177344
	2270186177344 [label=AccumulateGrad]
	2270186177488 -> 2270186177632
	2270162322032 [label="stages.1.blocks.1.mlp.fc2.weight
 (80, 320, 1, 1)" fillcolor=lightblue]
	2270162322032 -> 2270186177488
	2270186177488 [label=AccumulateGrad]
	2270186177440 -> 2270186177632
	2270162322112 [label="stages.1.blocks.1.mlp.fc2.bias
 (80)" fillcolor=lightblue]
	2270162322112 -> 2270186177440
	2270186177440 [label=AccumulateGrad]
	2270186177728 -> 2270186177872
	2270186177728 [label=ReshapeAliasBackward0]
	2270186176816 -> 2270186177728
	2270162321952 [label="stages.1.blocks.1.gamma
 (80)" fillcolor=lightblue]
	2270162321952 -> 2270186176816
	2270186176816 [label=AccumulateGrad]
	2270186177968 -> 2270186177920
	2270186178352 -> 2270186178304
	2270162322272 [label="stages.2.downsample.0.weight
 (80)" fillcolor=lightblue]
	2270162322272 -> 2270186178352
	2270186178352 [label=AccumulateGrad]
	2270186178592 -> 2270186178304
	2270162322352 [label="stages.2.downsample.0.bias
 (80)" fillcolor=lightblue]
	2270162322352 -> 2270186178592
	2270186178592 [label=AccumulateGrad]
	2270186178736 -> 2270186180512
	2270162322592 [label="stages.2.downsample.1.weight
 (160, 80, 2, 2)" fillcolor=lightblue]
	2270162322592 -> 2270186178736
	2270186178736 [label=AccumulateGrad]
	2270186178688 -> 2270186180512
	2270162322672 [label="stages.2.downsample.1.bias
 (160)" fillcolor=lightblue]
	2270162322672 -> 2270186178688
	2270186178688 [label=AccumulateGrad]
	2270186178880 -> 2270186178928
	2270162322832 [label="stages.2.blocks.0.conv_dw.weight
 (160, 1, 7, 7)" fillcolor=lightblue]
	2270162322832 -> 2270186178880
	2270186178880 [label=AccumulateGrad]
	2270186178976 -> 2270186178928
	2270162322912 [label="stages.2.blocks.0.conv_dw.bias
 (160)" fillcolor=lightblue]
	2270162322912 -> 2270186178976
	2270186178976 [label=AccumulateGrad]
	2270186179360 -> 2270186179312
	2270162322752 [label="stages.2.blocks.0.norm.weight
 (160)" fillcolor=lightblue]
	2270162322752 -> 2270186179360
	2270186179360 [label=AccumulateGrad]
	2270186179600 -> 2270186179312
	2270162322992 [label="stages.2.blocks.0.norm.bias
 (160)" fillcolor=lightblue]
	2270162322992 -> 2270186179600
	2270186179600 [label=AccumulateGrad]
	2270186179840 -> 2270186179792
	2270162323152 [label="stages.2.blocks.0.mlp.fc1.weight
 (640, 160, 1, 1)" fillcolor=lightblue]
	2270162323152 -> 2270186179840
	2270186179840 [label=AccumulateGrad]
	2270186180080 -> 2270186179792
	2270162323232 [label="stages.2.blocks.0.mlp.fc1.bias
 (640)" fillcolor=lightblue]
	2270162323232 -> 2270186180080
	2270186180080 [label=AccumulateGrad]
	2270186180224 -> 2270186180368
	2270162323392 [label="stages.2.blocks.0.mlp.fc2.weight
 (160, 640, 1, 1)" fillcolor=lightblue]
	2270162323392 -> 2270186180224
	2270186180224 [label=AccumulateGrad]
	2270186180176 -> 2270186180368
	2270162323472 [label="stages.2.blocks.0.mlp.fc2.bias
 (160)" fillcolor=lightblue]
	2270162323472 -> 2270186180176
	2270186180176 [label=AccumulateGrad]
	2270186180464 -> 2270186197056
	2270186180464 [label=ReshapeAliasBackward0]
	2270186179552 -> 2270186180464
	2270162323312 [label="stages.2.blocks.0.gamma
 (160)" fillcolor=lightblue]
	2270162323312 -> 2270186179552
	2270186179552 [label=AccumulateGrad]
	2270186180512 -> 2270186199024
	2270186197200 -> 2270186197248
	2270162323792 [label="stages.2.blocks.1.conv_dw.weight
 (160, 1, 7, 7)" fillcolor=lightblue]
	2270162323792 -> 2270186197200
	2270186197200 [label=AccumulateGrad]
	2270186197296 -> 2270186197248
	2270162323872 [label="stages.2.blocks.1.conv_dw.bias
 (160)" fillcolor=lightblue]
	2270162323872 -> 2270186197296
	2270186197296 [label=AccumulateGrad]
	2270186197776 -> 2270186197728
	2270162323712 [label="stages.2.blocks.1.norm.weight
 (160)" fillcolor=lightblue]
	2270162323712 -> 2270186197776
	2270186197776 [label=AccumulateGrad]
	2270186197920 -> 2270186197728
	2270162323952 [label="stages.2.blocks.1.norm.bias
 (160)" fillcolor=lightblue]
	2270162323952 -> 2270186197920
	2270186197920 [label=AccumulateGrad]
	2270186198160 -> 2270186198112
	2270162324112 [label="stages.2.blocks.1.mlp.fc1.weight
 (640, 160, 1, 1)" fillcolor=lightblue]
	2270162324112 -> 2270186198160
	2270186198160 [label=AccumulateGrad]
	2270186198400 -> 2270186198112
	2270162324192 [label="stages.2.blocks.1.mlp.fc1.bias
 (640)" fillcolor=lightblue]
	2270162324192 -> 2270186198400
	2270186198400 [label=AccumulateGrad]
	2270186198544 -> 2270186198688
	2270162324352 [label="stages.2.blocks.1.mlp.fc2.weight
 (160, 640, 1, 1)" fillcolor=lightblue]
	2270162324352 -> 2270186198544
	2270186198544 [label=AccumulateGrad]
	2270186198496 -> 2270186198688
	2270162324432 [label="stages.2.blocks.1.mlp.fc2.bias
 (160)" fillcolor=lightblue]
	2270162324432 -> 2270186198496
	2270186198496 [label=AccumulateGrad]
	2270186198784 -> 2270186198928
	2270186198784 [label=ReshapeAliasBackward0]
	2270186197872 -> 2270186198784
	2270162324272 [label="stages.2.blocks.1.gamma
 (160)" fillcolor=lightblue]
	2270162324272 -> 2270186197872
	2270186197872 [label=AccumulateGrad]
	2270186199024 -> 2270186200896
	2270186199168 -> 2270186199216
	2270162324752 [label="stages.2.blocks.2.conv_dw.weight
 (160, 1, 7, 7)" fillcolor=lightblue]
	2270162324752 -> 2270186199168
	2270186199168 [label=AccumulateGrad]
	2270186199264 -> 2270186199216
	2270162324832 [label="stages.2.blocks.2.conv_dw.bias
 (160)" fillcolor=lightblue]
	2270162324832 -> 2270186199264
	2270186199264 [label=AccumulateGrad]
	2270186199648 -> 2270186199600
	2270162324672 [label="stages.2.blocks.2.norm.weight
 (160)" fillcolor=lightblue]
	2270162324672 -> 2270186199648
	2270186199648 [label=AccumulateGrad]
	2270186199792 -> 2270186199600
	2270162324912 [label="stages.2.blocks.2.norm.bias
 (160)" fillcolor=lightblue]
	2270162324912 -> 2270186199792
	2270186199792 [label=AccumulateGrad]
	2270186200032 -> 2270186199984
	2270162325072 [label="stages.2.blocks.2.mlp.fc1.weight
 (640, 160, 1, 1)" fillcolor=lightblue]
	2270162325072 -> 2270186200032
	2270186200032 [label=AccumulateGrad]
	2270186200272 -> 2270186199984
	2270162325152 [label="stages.2.blocks.2.mlp.fc1.bias
 (640)" fillcolor=lightblue]
	2270162325152 -> 2270186200272
	2270186200272 [label=AccumulateGrad]
	2270186200416 -> 2270186200560
	2270162325312 [label="stages.2.blocks.2.mlp.fc2.weight
 (160, 640, 1, 1)" fillcolor=lightblue]
	2270162325312 -> 2270186200416
	2270186200416 [label=AccumulateGrad]
	2270186200368 -> 2270186200560
	2270162325392 [label="stages.2.blocks.2.mlp.fc2.bias
 (160)" fillcolor=lightblue]
	2270162325392 -> 2270186200368
	2270186200368 [label=AccumulateGrad]
	2270186200656 -> 2270186200800
	2270186200656 [label=ReshapeAliasBackward0]
	2270186199744 -> 2270186200656
	2270162325232 [label="stages.2.blocks.2.gamma
 (160)" fillcolor=lightblue]
	2270162325232 -> 2270186199744
	2270186199744 [label=AccumulateGrad]
	2270186200896 -> 2270186219216
	2270186217584 -> 2270186217536
	2270179066160 [label="stages.2.blocks.3.conv_dw.weight
 (160, 1, 7, 7)" fillcolor=lightblue]
	2270179066160 -> 2270186217584
	2270186217584 [label=AccumulateGrad]
	2270186217728 -> 2270186217536
	2270179066240 [label="stages.2.blocks.3.conv_dw.bias
 (160)" fillcolor=lightblue]
	2270179066240 -> 2270186217728
	2270186217728 [label=AccumulateGrad]
	2270186217968 -> 2270186217920
	2270179066080 [label="stages.2.blocks.3.norm.weight
 (160)" fillcolor=lightblue]
	2270179066080 -> 2270186217968
	2270186217968 [label=AccumulateGrad]
	2270186218112 -> 2270186217920
	2270179066320 [label="stages.2.blocks.3.norm.bias
 (160)" fillcolor=lightblue]
	2270179066320 -> 2270186218112
	2270186218112 [label=AccumulateGrad]
	2270186218352 -> 2270186218304
	2270179066480 [label="stages.2.blocks.3.mlp.fc1.weight
 (640, 160, 1, 1)" fillcolor=lightblue]
	2270179066480 -> 2270186218352
	2270186218352 [label=AccumulateGrad]
	2270186218592 -> 2270186218304
	2270179066560 [label="stages.2.blocks.3.mlp.fc1.bias
 (640)" fillcolor=lightblue]
	2270179066560 -> 2270186218592
	2270186218592 [label=AccumulateGrad]
	2270186218736 -> 2270186218880
	2270179066720 [label="stages.2.blocks.3.mlp.fc2.weight
 (160, 640, 1, 1)" fillcolor=lightblue]
	2270179066720 -> 2270186218736
	2270186218736 [label=AccumulateGrad]
	2270186218688 -> 2270186218880
	2270179066800 [label="stages.2.blocks.3.mlp.fc2.bias
 (160)" fillcolor=lightblue]
	2270179066800 -> 2270186218688
	2270186218688 [label=AccumulateGrad]
	2270186218976 -> 2270186219120
	2270186218976 [label=ReshapeAliasBackward0]
	2270186218064 -> 2270186218976
	2270179066640 [label="stages.2.blocks.3.gamma
 (160)" fillcolor=lightblue]
	2270179066640 -> 2270186218064
	2270186218064 [label=AccumulateGrad]
	2270186219216 -> 2270186221088
	2270186219360 -> 2270186219408
	2270179067120 [label="stages.2.blocks.4.conv_dw.weight
 (160, 1, 7, 7)" fillcolor=lightblue]
	2270179067120 -> 2270186219360
	2270186219360 [label=AccumulateGrad]
	2270186219456 -> 2270186219408
	2270179067200 [label="stages.2.blocks.4.conv_dw.bias
 (160)" fillcolor=lightblue]
	2270179067200 -> 2270186219456
	2270186219456 [label=AccumulateGrad]
	2270186219840 -> 2270186219792
	2270179067040 [label="stages.2.blocks.4.norm.weight
 (160)" fillcolor=lightblue]
	2270179067040 -> 2270186219840
	2270186219840 [label=AccumulateGrad]
	2270186219984 -> 2270186219792
	2270179067280 [label="stages.2.blocks.4.norm.bias
 (160)" fillcolor=lightblue]
	2270179067280 -> 2270186219984
	2270186219984 [label=AccumulateGrad]
	2270186220224 -> 2270186220176
	2270179067440 [label="stages.2.blocks.4.mlp.fc1.weight
 (640, 160, 1, 1)" fillcolor=lightblue]
	2270179067440 -> 2270186220224
	2270186220224 [label=AccumulateGrad]
	2270186220464 -> 2270186220176
	2270179067520 [label="stages.2.blocks.4.mlp.fc1.bias
 (640)" fillcolor=lightblue]
	2270179067520 -> 2270186220464
	2270186220464 [label=AccumulateGrad]
	2270186220608 -> 2270186220752
	2270179067680 [label="stages.2.blocks.4.mlp.fc2.weight
 (160, 640, 1, 1)" fillcolor=lightblue]
	2270179067680 -> 2270186220608
	2270186220608 [label=AccumulateGrad]
	2270186220560 -> 2270186220752
	2270179067760 [label="stages.2.blocks.4.mlp.fc2.bias
 (160)" fillcolor=lightblue]
	2270179067760 -> 2270186220560
	2270186220560 [label=AccumulateGrad]
	2270186220848 -> 2270186220992
	2270186220848 [label=ReshapeAliasBackward0]
	2270186219936 -> 2270186220848
	2270179067600 [label="stages.2.blocks.4.gamma
 (160)" fillcolor=lightblue]
	2270179067600 -> 2270186219936
	2270186219936 [label=AccumulateGrad]
	2270186221088 -> 2270186247600
	2270186221232 -> 2270186221280
	2270179068080 [label="stages.2.blocks.5.conv_dw.weight
 (160, 1, 7, 7)" fillcolor=lightblue]
	2270179068080 -> 2270186221232
	2270186221232 [label=AccumulateGrad]
	2270186221328 -> 2270186221280
	2270179068160 [label="stages.2.blocks.5.conv_dw.bias
 (160)" fillcolor=lightblue]
	2270179068160 -> 2270186221328
	2270186221328 [label=AccumulateGrad]
	2270186246352 -> 2270186246304
	2270179068000 [label="stages.2.blocks.5.norm.weight
 (160)" fillcolor=lightblue]
	2270179068000 -> 2270186246352
	2270186246352 [label=AccumulateGrad]
	2270186246496 -> 2270186246304
	2270179068240 [label="stages.2.blocks.5.norm.bias
 (160)" fillcolor=lightblue]
	2270179068240 -> 2270186246496
	2270186246496 [label=AccumulateGrad]
	2270186246736 -> 2270186246688
	2270179068400 [label="stages.2.blocks.5.mlp.fc1.weight
 (640, 160, 1, 1)" fillcolor=lightblue]
	2270179068400 -> 2270186246736
	2270186246736 [label=AccumulateGrad]
	2270186246976 -> 2270186246688
	2270179068480 [label="stages.2.blocks.5.mlp.fc1.bias
 (640)" fillcolor=lightblue]
	2270179068480 -> 2270186246976
	2270186246976 [label=AccumulateGrad]
	2270186247120 -> 2270186247264
	2270179068640 [label="stages.2.blocks.5.mlp.fc2.weight
 (160, 640, 1, 1)" fillcolor=lightblue]
	2270179068640 -> 2270186247120
	2270186247120 [label=AccumulateGrad]
	2270186247072 -> 2270186247264
	2270179068720 [label="stages.2.blocks.5.mlp.fc2.bias
 (160)" fillcolor=lightblue]
	2270179068720 -> 2270186247072
	2270186247072 [label=AccumulateGrad]
	2270186247360 -> 2270186247504
	2270186247360 [label=ReshapeAliasBackward0]
	2270186246448 -> 2270186247360
	2270179068560 [label="stages.2.blocks.5.gamma
 (160)" fillcolor=lightblue]
	2270179068560 -> 2270186246448
	2270186246448 [label=AccumulateGrad]
	2270186247600 -> 2270186247552
	2270186247984 -> 2270186247936
	2270179068880 [label="stages.3.downsample.0.weight
 (160)" fillcolor=lightblue]
	2270179068880 -> 2270186247984
	2270186247984 [label=AccumulateGrad]
	2270186248224 -> 2270186247936
	2270179068960 [label="stages.3.downsample.0.bias
 (160)" fillcolor=lightblue]
	2270179068960 -> 2270186248224
	2270186248224 [label=AccumulateGrad]
	2270186248368 -> 2270186250144
	2270179069200 [label="stages.3.downsample.1.weight
 (320, 160, 2, 2)" fillcolor=lightblue]
	2270179069200 -> 2270186248368
	2270186248368 [label=AccumulateGrad]
	2270186248320 -> 2270186250144
	2270179069280 [label="stages.3.downsample.1.bias
 (320)" fillcolor=lightblue]
	2270179069280 -> 2270186248320
	2270186248320 [label=AccumulateGrad]
	2270186248512 -> 2270186248560
	2270179069440 [label="stages.3.blocks.0.conv_dw.weight
 (320, 1, 7, 7)" fillcolor=lightblue]
	2270179069440 -> 2270186248512
	2270186248512 [label=AccumulateGrad]
	2270186248608 -> 2270186248560
	2270179069520 [label="stages.3.blocks.0.conv_dw.bias
 (320)" fillcolor=lightblue]
	2270179069520 -> 2270186248608
	2270186248608 [label=AccumulateGrad]
	2270186248992 -> 2270186248944
	2270179069360 [label="stages.3.blocks.0.norm.weight
 (320)" fillcolor=lightblue]
	2270179069360 -> 2270186248992
	2270186248992 [label=AccumulateGrad]
	2270186249232 -> 2270186248944
	2270179069600 [label="stages.3.blocks.0.norm.bias
 (320)" fillcolor=lightblue]
	2270179069600 -> 2270186249232
	2270186249232 [label=AccumulateGrad]
	2270186249472 -> 2270186249424
	2270179069760 [label="stages.3.blocks.0.mlp.fc1.weight
 (1280, 320, 1, 1)" fillcolor=lightblue]
	2270179069760 -> 2270186249472
	2270186249472 [label=AccumulateGrad]
	2270186249712 -> 2270186249424
	2270179069840 [label="stages.3.blocks.0.mlp.fc1.bias
 (1280)" fillcolor=lightblue]
	2270179069840 -> 2270186249712
	2270186249712 [label=AccumulateGrad]
	2270186249856 -> 2270186250000
	2270179221648 [label="stages.3.blocks.0.mlp.fc2.weight
 (320, 1280, 1, 1)" fillcolor=lightblue]
	2270179221648 -> 2270186249856
	2270186249856 [label=AccumulateGrad]
	2270186249808 -> 2270186250000
	2270179221728 [label="stages.3.blocks.0.mlp.fc2.bias
 (320)" fillcolor=lightblue]
	2270179221728 -> 2270186249808
	2270186249808 [label=AccumulateGrad]
	2270186250096 -> 2270186262592
	2270186250096 [label=ReshapeAliasBackward0]
	2270186249184 -> 2270186250096
	2270179221568 [label="stages.3.blocks.0.gamma
 (320)" fillcolor=lightblue]
	2270179221568 -> 2270186249184
	2270186249184 [label=AccumulateGrad]
	2270186250144 -> 2270186264560
	2270186262736 -> 2270186262784
	2270179222048 [label="stages.3.blocks.1.conv_dw.weight
 (320, 1, 7, 7)" fillcolor=lightblue]
	2270179222048 -> 2270186262736
	2270186262736 [label=AccumulateGrad]
	2270186262832 -> 2270186262784
	2270179222128 [label="stages.3.blocks.1.conv_dw.bias
 (320)" fillcolor=lightblue]
	2270179222128 -> 2270186262832
	2270186262832 [label=AccumulateGrad]
	2270186263312 -> 2270186263264
	2270179221968 [label="stages.3.blocks.1.norm.weight
 (320)" fillcolor=lightblue]
	2270179221968 -> 2270186263312
	2270186263312 [label=AccumulateGrad]
	2270186263456 -> 2270186263264
	2270179222208 [label="stages.3.blocks.1.norm.bias
 (320)" fillcolor=lightblue]
	2270179222208 -> 2270186263456
	2270186263456 [label=AccumulateGrad]
	2270186263696 -> 2270186263648
	2270179222368 [label="stages.3.blocks.1.mlp.fc1.weight
 (1280, 320, 1, 1)" fillcolor=lightblue]
	2270179222368 -> 2270186263696
	2270186263696 [label=AccumulateGrad]
	2270186263936 -> 2270186263648
	2270179222448 [label="stages.3.blocks.1.mlp.fc1.bias
 (1280)" fillcolor=lightblue]
	2270179222448 -> 2270186263936
	2270186263936 [label=AccumulateGrad]
	2270186264080 -> 2270186264224
	2270179222608 [label="stages.3.blocks.1.mlp.fc2.weight
 (320, 1280, 1, 1)" fillcolor=lightblue]
	2270179222608 -> 2270186264080
	2270186264080 [label=AccumulateGrad]
	2270186264032 -> 2270186264224
	2270179222688 [label="stages.3.blocks.1.mlp.fc2.bias
 (320)" fillcolor=lightblue]
	2270179222688 -> 2270186264032
	2270186264032 [label=AccumulateGrad]
	2270186264320 -> 2270186264464
	2270186264320 [label=ReshapeAliasBackward0]
	2270186263408 -> 2270186264320
	2270179222528 [label="stages.3.blocks.1.gamma
 (320)" fillcolor=lightblue]
	2270179222528 -> 2270186263408
	2270186263408 [label=AccumulateGrad]
	2270186264560 -> 2270186264512
	2270186265136 -> 2270186265232
	2270179222848 [label="head.norm.weight
 (320)" fillcolor=lightblue]
	2270179222848 -> 2270186265136
	2270186265136 [label=AccumulateGrad]
	2270186265280 -> 2270186265232
	2270179222928 [label="head.norm.bias
 (320)" fillcolor=lightblue]
	2270179222928 -> 2270186265280
	2270186265280 [label=AccumulateGrad]
	2270186265568 -> 2270186265904
	2270186265568 [label=TBackward0]
	2270186265088 -> 2270186265568
	2270179223008 [label="head.fc.weight
 (10, 320)" fillcolor=lightblue]
	2270179223008 -> 2270186265088
	2270186265088 [label=AccumulateGrad]
	2270186265856 -> 2270186190224
}
